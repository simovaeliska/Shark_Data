{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "fb9c1997-e077-4f93-ac9f-86e44eccb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_string(input_string):\n",
    "    # Return an empty string if input is NaN or not a string\n",
    "    if pd.isna(input_string) or not isinstance(input_string, str):\n",
    "        return ''\n",
    "    \n",
    "    # Keep only numerical characters and hyphens\n",
    "    cleaned_string = ''.join(char for char in input_string if char.isdigit() or char == '-')\n",
    "    return cleaned_string\n",
    "\n",
    "#Owen - date cleaning\n",
    "def clean_dates2(df):\n",
    "    \"\"\" Goes through a column in the DataFrame called \"Date\" and cleans\n",
    "    Removes \"reported\" from the column to just get the date\n",
    "    Deleted any trailing spaces, and makes sure the format is just numbers and hyphens\n",
    "    Converts the column to the pandas datetime format\n",
    "    Removes rows from the data with invalid dates\n",
    "    Filters to only include dates after 1900 and dates with years that match the \"Year\" column\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    #6973 rows\n",
    "    #Removes \"reported\" from dates and trailing spaces\n",
    "    new_df['Date'] = new_df['Date'].str.replace('reported', '', case=False)\n",
    "    new_df[\"Date\"] = new_df[\"Date\"].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    new_df['Date'] = new_df['Date'].str.replace(' ', '-', regex=False)\n",
    "    new_df['Date'] = new_df['Date'].str.replace(r'-+', '-', regex=True)\n",
    "\n",
    "    #converts to date format\n",
    "    new_df['Date'] = pd.to_datetime(new_df['Date'], errors='coerce')\n",
    "\n",
    "    #drop rows invalid dates\n",
    "    new_df.dropna(subset=['Date'], inplace=True)\n",
    "    #5376 rows\n",
    "\n",
    "    #drop years before 1900\n",
    "    new_df = new_df[new_df[\"Year\"] >= 1900 ]\n",
    "    new_df = new_df[new_df['Date'] >= pd.Timestamp('1900-01-01')].reset_index(drop=True)\n",
    "    #5114 rows\n",
    "\n",
    "    # Check year from date against Year column\n",
    "    if 'Year' in new_df.columns:\n",
    "        new_df = new_df[new_df['Year'] == new_df['Date'].dt.year]\n",
    "    return new_df\n",
    "\n",
    "#Eliska\n",
    "def clean_states(df: pd.DataFrame):\n",
    "    \"\"\" Tidies the \"State\" column\n",
    "    Converts \"State\" column to lower case \n",
    "    Only keeps States with at least 5 cases\n",
    "    Uses a dictionary to update State names to cleaned versions where there were errors\n",
    "    Converts back to title case\n",
    "    \"\"\"\n",
    "    version_2 = df.copy() #independent copy so we wont mess the potential DF\n",
    "    version_2 = version_2.dropna(subset=[\"State\"]) #get rid of empty values\n",
    "    version_2.loc[:, \"State\"] = version_2[\"State\"].str.lower() #convert to lowercase\n",
    "\n",
    "    #get rid of countries that occur 5 or less times\n",
    "    state_counts = version_2[\"State\"].value_counts()\n",
    "    threshold = 5\n",
    "    states_to_keep = state_counts[state_counts >= threshold].index\n",
    "    version_2 = version_2[version_2[\"State\"].isin(states_to_keep)]\n",
    "\n",
    "    #corrections\n",
    "    state_corrections = {\"westerm australia\": \"western australia\", \"western australia\" : \"western australia\",\n",
    "                        \"mirs bay \": \"mirs bay\", \"mirs bay\" : \"mirs bay\",\n",
    "                        \"baja california\" : \"california\",\n",
    "                        \" primorje-gorski kotar county\": \"primorje-gorski kotar county\",\n",
    "                        }\n",
    "    version_2[\"State\"] = version_2[\"State\"].replace(state_corrections) # apply corrections\n",
    "    version_2[\"State\"] = version_2[\"State\"].str.title() #get back the capital letter of each word in states\n",
    "\n",
    "    return version_2\n",
    "\n",
    "#Owen\n",
    "def clean_cols(df: pd.DataFrame):\n",
    "    \"\"\"Removes empty columns and names \"Fatal\" correctly \"\"\"\n",
    "    new_df = df.rename(columns={'Unnamed: 11': 'Fatal'})\n",
    "    new_df = new_df.drop(['href formula', 'href','Case Number', 'Case Number.1',\n",
    "       'original order', 'Unnamed: 21', 'Unnamed: 22', \"pdf\"], axis=1)\n",
    "    new_df = new_df.drop_duplicates()\n",
    "    return new_df\n",
    "\n",
    "#Constanza\n",
    "def clean_type(df: pd.DataFrame):\n",
    "    \"\"\" Cleans \"type\" column\n",
    "    Removes anything other than \"Provoked\" and \"Unprovoked\", and changes to \"Unknown\"\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    new_df['Type'] = new_df['Type'].replace({' Provoked': 'Provoked'})\n",
    "\n",
    "    values_to_replace = ['Questionable', 'Watercraft', 'Sea Disaster', '?', 'Unconfirmed', 'Unverified', 'Invalid', 'Under investigation', 'Boat']\n",
    "    new_df['Type'] = new_df['Type'].replace(values_to_replace, 'Unknown')\n",
    "\n",
    "    return new_df\n",
    "\n",
    "#Owen\n",
    "def clean_country(df):\n",
    "    \"\"\"Tidies the \"Country\" column of the DataFrama\n",
    "    Converts all to title case, and strips trailing spaces\n",
    "    Usa then converted back to USA\n",
    "    Removes data for Oceans and Seas, which are not countries\n",
    "    Replaces some common errors to their correct Country names (eg. Ceylon to Sri Lanka)\n",
    "    Removes rows with Countries that only have one incident\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    new_df = new_df.dropna(subset=[\"Country\"])\n",
    "\n",
    "    # Converts country column to consistent capitalisation and strips spaces\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].apply(lambda x: x.strip().title())\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].apply(lambda x: \"USA\" if x == \"Usa\" else x)\n",
    "\n",
    "    #Removes rows that contain Oceans and Seas for the country\n",
    "    new_df = new_df[~new_df[\"Country\"].str.contains(\"Ocean\", na=False)]\n",
    "    new_df = new_df[~new_df[\"Country\"].str.contains(\"Central Pacific\", na=False)]\n",
    "    new_df = new_df[~new_df[\"Country\"].str.contains(\" Sea\", na=False)]\n",
    "    new_df = new_df[~new_df[\"Country\"].str.contains(\"Persian Gulf\", na=False)]\n",
    "\n",
    "    #Corrects country names\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Ceylon (Sri Lanka)\", \"Sri Lanka\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Ceylon\", \"Sri Lanka\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Maldive Islands\", \"Maldives\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"St. Maartin\", \"St Martin\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"St. Martin\", \"St Martin\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Reunion Island\", \"Reunion\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Trinidad\", \"Trinidad & Tobago\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Tobago\", \"Trinidad & Tobago\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Turks And Caicos\", \"Turks & Caicos\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Sudan?\", \"Sudan\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"United Arab Emirates (Uae)?\", \"United Arab Emirates\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"United Arab Emirates (Uae)\", \"United Arab Emirates\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Western Samoa\", \"Samoa\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Scotland\", \"United Kingdom\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Crete\", \"Greece\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Okinawa\", \"Japan\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"Columbia\", \"Colombia\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"England\", \"United Kingdom\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"New Britain\", \"Papua New Guinea\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace(\"New Guinea\", \"Papua New Guinea\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace('St Helena, British Overseas Territory', \"St Helena\")\n",
    "    new_df[\"Country\"] = new_df[\"Country\"].replace('Burma', \"Myanmar\")\n",
    "\n",
    "    #Counts occurences of each country\n",
    "    country_counts = new_df[\"Country\"].value_counts() #Contains 6923\n",
    "\n",
    "\n",
    "    # Filter countries that appear more than two times\n",
    "    countries_to_keep = country_counts[country_counts > 1].index\n",
    "    new_df = new_df[new_df[\"Country\"].isin(countries_to_keep)]\n",
    "\n",
    "    return new_df\n",
    "\n",
    "#Owen\n",
    "def hemisphere(df: pd.DataFrame):\n",
    "    \"\"\"Adds in a \"Hemisphere\" column\n",
    "    This uses a dictionary listing whether each country is in the \"North\", \"South\" or on the \"Equator\"\n",
    "    If a country is missing from this dictionary it returns \"Na\"\n",
    "    \"\"\"\n",
    "\n",
    "    new_df = df.copy()\n",
    "\n",
    "    hemi_dict = {\n",
    "        'American Samoa': \"South\",\n",
    "        'Antigua': 'North',\n",
    "        'Argentina': \"South\",\n",
    "        'Australia': \"South\",\n",
    "        'Azores': \"North\",\n",
    "        'Bahamas': \"North\",\n",
    "        'Barbados': \"North\",\n",
    "        'Belize': \"North\",\n",
    "        'Bermuda': \"North\",\n",
    "        'Brazil': \"Equator\",\n",
    "        'Myanmar': \"North\",\n",
    "        'Canada': \"North\",\n",
    "        'Cape Verde': \"North\",\n",
    "        'Cayman Islands': \"North\",\n",
    "        'Chile': \"South\",\n",
    "        'China': \"North\",\n",
    "        'Colombia': \"Equator\",\n",
    "        'Costa Rica': \"North\",\n",
    "        'Croatia': \"North\",\n",
    "        'Cuba': \"North\",\n",
    "        'Dominican Republic': \"North\",\n",
    "        'Ecuador': \"Equator\",\n",
    "        'Egypt': \"North\",\n",
    "        'El Salvador': \"North\",\n",
    "        'Fiji': \"South\",\n",
    "        'France': \"North\",\n",
    "        'French Polynesia': \"South\",\n",
    "        'Greece': \"North\",\n",
    "        'Grenada': \"North\",\n",
    "        'Guam': \"North\",\n",
    "        'Guinea': \"North\",\n",
    "        'Guyana': \"North\",\n",
    "        'Haiti': \"North\",\n",
    "        'Honduras': \"North\",\n",
    "        'Hong Kong': \"North\",\n",
    "        'Iceland': \"North\",\n",
    "        'India': \"North\",\n",
    "        'Indonesia': \"Equator\",\n",
    "        'Iran': \"North\",\n",
    "        'Iraq': \"North\",\n",
    "        'Ireland': \"North\",\n",
    "        'Israel': \"North\",\n",
    "        'Italy': \"North\",\n",
    "        'Jamaica': \"North\",\n",
    "        'Japan': \"North\",\n",
    "        'Johnston Island': \"North\",\n",
    "        'Kenya': \"Equator\",\n",
    "        'Kiribati': \"Equator\",\n",
    "        'Lebanon': \"North\",\n",
    "        'Liberia': \"North\",\n",
    "        'Libya': \"North\",\n",
    "        'Madagascar': \"South\",\n",
    "        'Malaysia': \"North\",\n",
    "        'Maldives': \"Equator\",\n",
    "        'Malta': \"North\",\n",
    "        'Marshall Islands': \"North\",\n",
    "        'Martinique': \"North\",\n",
    "        'Mauritius': \"South\",\n",
    "        'Mexico': \"North\",\n",
    "        'Micronesia': \"North\",\n",
    "        'Montenegro': \"North\",\n",
    "        'Mozambique': \"South\",\n",
    "        'Namibia': \"South\",\n",
    "        'New Caledonia': \"South\",\n",
    "        'New Zealand': \"South\",\n",
    "        'Nicaragua': \"North\",\n",
    "        'Nigeria': \"North\",\n",
    "        'Norway': \"North\",\n",
    "        'Palau': \"North\",\n",
    "        'Panama': \"North\",\n",
    "        'Papua New Guinea': \"South\",\n",
    "        'Peru': \"South\",\n",
    "        'Philippines': \"North\",\n",
    "        'Portugal': \"North\",\n",
    "        'Reunion': \"South\",\n",
    "        'Russia': \"North\",\n",
    "        'Samoa': \"South\",\n",
    "        'Saudi Arabia': \"North\",\n",
    "        'Senegal': \"North\",\n",
    "        'Seychelles': \"South\",\n",
    "        'Sierra Leone': \"North\",\n",
    "        'Singapore': \"North\",\n",
    "        'Solomon Islands': \"South\",\n",
    "        'Somalia': \"Equator\",\n",
    "        'South Africa': \"South\",\n",
    "        'South Korea': \"North\",\n",
    "        'Spain': \"North\",\n",
    "        'Sri Lanka': \"South\",\n",
    "        'St Helena, British Overseas Territory': \"South\",\n",
    "        'St Martin': \"North\",\n",
    "        'St Helena': \"South\",\n",
    "        'Sudan': \"North\",\n",
    "        'Taiwan': \"North\",\n",
    "        'Tanzania': \"Equator\",\n",
    "        'Thailand': \"North\",\n",
    "        'Tonga': \"South\",\n",
    "        'Trinidad & Tobago': \"North\",\n",
    "        'Tunisia': \"North\",\n",
    "        'Turkey': \"North\",\n",
    "        'Turks & Caicos': \"North\",\n",
    "        'USA': \"North\",\n",
    "        'United Arab Emirates': \"North\",\n",
    "        'United Kingdom': \"North\",\n",
    "        'Uruguay': \"South\",\n",
    "        'Vanuatu': \"South\",\n",
    "        'Venezuela': \"North\",\n",
    "        'Vietnam': \"North\",\n",
    "        'West Indies': \"North\",\n",
    "        'Yemen': \"North\"\n",
    "    }\n",
    "\n",
    "    #is assigning a \"Hemisphere\" column to new_df by mapping each entry in the \"Country\" column to a hemisphere based on a dictionary, hemi_dict.\n",
    "    new_df[\"Hemisphere\"] = new_df[\"Country\"].apply(lambda country: hemi_dict.get(country, \"Na\"))\n",
    "\n",
    "    #Code here\n",
    "    return new_df\n",
    "\n",
    "#filip\n",
    "def clean_sex(df):\n",
    "    \"\"\" Uses a dictionary to replace the common errors in the Sex column with \"M\" or \"F\" \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2[\"Sex\"] = df2[\"Sex\"].replace({ ' M': 'M', 'M ': 'M', 'M x 2': 'M',})\n",
    "    df2[\"Sex\"] = df2[\"Sex\"].replace(['.', 'lli', 'N'], np.nan)\n",
    "    return df2\n",
    "\n",
    "#filip\n",
    "def clean_age(shark_df):\n",
    "    \"\"\" Uses a dictionary to replace the common errors in the Age column with appropriate values or NaN if unclear \"\"\"\n",
    "    shark_df[\"Age\"] = shark_df[\"Age\"].replace({\n",
    "        '30s': '30',\n",
    "        '20/30': '25',\n",
    "        '20s': '20',\n",
    "        '50s': '50',\n",
    "        '40s': '40',\n",
    "        '60s': '60',\n",
    "        \"20's\": '20',\n",
    "        '18 months': '2',\n",
    "        '18 or 20': '19',\n",
    "        '12 or 13': '13',\n",
    "        '8 or 10': '9',\n",
    "        '30 or 36': '33',\n",
    "        '6½': '6',\n",
    "        '21 & ?': '21',\n",
    "        '33 or 37': '35',\n",
    "        'mid-30s': '35',\n",
    "        '23 & 20': '21',\n",
    "        '28': '28',\n",
    "        '20?': '20',\n",
    "        \"60's\": '62',\n",
    "        '32 & 30': '31',\n",
    "        '16 to 18': '17',\n",
    "        'mid-20s': '25',\n",
    "        'Ca. 33': '33',\n",
    "        '45 ': '45',\n",
    "        '21 or 26': '24',\n",
    "        '20 ': '20',\n",
    "        '>50': '55',\n",
    "        '18 to 22': '20',\n",
    "        '9 & 12': '10',\n",
    "        '? & 19': '19',\n",
    "        '9 months': '1',\n",
    "        '25 to 35': '30',\n",
    "        '23 & 26': '24',\n",
    "        '33 & 37': '35',\n",
    "        '25 or 28': '26',\n",
    "        '30 & 32': '31',\n",
    "        '50 & 30': '40',\n",
    "        '13 or 18': '16',\n",
    "        '34 & 19': '31',\n",
    "        '33 & 26': '30',\n",
    "        '2 to 3 months': '1',\n",
    "        '43': '43',\n",
    "        '7 or 8': '8',\n",
    "        '17 & 16': '17',\n",
    "        'Both 11': '11',\n",
    "        '9 or 10': '10',\n",
    "        '36 & 23': '30',\n",
    "        '10 or 12': '11',\n",
    "        '31 or 33': '32',\n",
    "        '2½': '2',\n",
    "        '13 or 14': '14'\n",
    "    })\n",
    "\n",
    "    shark_df[\"Age\"] = shark_df[\"Age\"].str.strip()\n",
    "    shark_df[\"Age\"] = shark_df[\"Age\"].replace([\n",
    "        'Middle age', np.nan, '?',\n",
    "        '!2', 'teen', 'Teen', '!6', '!!', '45 and 15', '28 & 22',\n",
    "        '9 & 60', 'a minor', '28 & 26', '46 & 34', '28, 23 & 30', 'Teens',\n",
    "        '36 & 26', '\\xa0', ' ', '7      &    31',\n",
    "        'Elderly', 'adult', '(adult)',\n",
    "        '37, 67, 35, 27, ? & 27', '21, 34,24 & 35', '17 & 35',\n",
    "        'X', '\"middle-age\"', 'MAKE LINE GREEN', '\"young\"', 'F',\n",
    "        'young', '  ', 'A.M.',\n",
    "           '?    &   14', 'M', '',\n",
    "    ], np.nan)\n",
    "    return(shark_df)\n",
    "\n",
    "def age_group(age):\n",
    "    \"\"\"Converts an input age to its appropriate group (eg. 0-9, 10-20, etc) \"\"\"\n",
    "    if pd.isna(age):\n",
    "        return \"Unknown\"\n",
    "    if age >= 100:\n",
    "        return \"100+\"\n",
    "    elif 90 <= age < 100:\n",
    "        return \"90-99\"\n",
    "    elif 80 <= age < 90:\n",
    "        return \"80-89\"\n",
    "    elif 70 <= age < 80:\n",
    "        return \"70-79\"\n",
    "    elif 60 <= age < 70:\n",
    "        return \"60-69\"\n",
    "    elif 50 <= age < 60:\n",
    "        return \"50-59\"\n",
    "    elif 40 <= age < 50:\n",
    "        return \"40-49\"\n",
    "    elif 30 <= age < 40:\n",
    "        return \"30-39\"\n",
    "    elif 20 <= age < 30:\n",
    "        return \"20-29\"\n",
    "    elif 10 <= age < 20:\n",
    "        return \"10-19\"\n",
    "    elif age < 10:\n",
    "        return \"0-9\"\n",
    "    else:\n",
    "        return \"Na\"\n",
    "\n",
    "def age_groups(shark_df):\n",
    "    \"\"\" Uses the age_group functin to create a new column in the DataFrame for Age Group \"\"\"\n",
    "    shark_df[\"Age\"] = pd.to_numeric(shark_df[\"Age\"], errors=\"coerce\")\n",
    "    shark_df[\"Age Group\"]= shark_df[\"Age\"].apply(age_group)\n",
    "    return shark_df\n",
    "\n",
    "def add_month(shark_df):\n",
    "    \"\"\" Uses the \"Date\" column to create a a new column for \"Month\" from these dates \"\"\"\n",
    "    \n",
    "    shark_df['Date'] = pd.to_datetime(shark_df['Date'], errors='coerce')\n",
    "    shark_df['Month'] = shark_df['Date'].dt.month\n",
    "    return shark_df\n",
    "\n",
    "def assign_season(row):\n",
    "    \"\"\" Uses the \"Hemisphere\" and \"Month\" columns to return what Season that is  \"\"\"\n",
    "    month = row['Month']\n",
    "    hemisphere = row['Hemisphere']\n",
    "    \n",
    "    if hemisphere == 'North':\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        elif month in [9, 10, 11]:\n",
    "            return 'Fall'\n",
    "    elif hemisphere == 'South':\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Summer'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Fall'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Winter'\n",
    "        elif month in [9, 10, 11]:\n",
    "            return 'Spring'\n",
    "    return 'Unknown'\n",
    "\n",
    "def add_season(shark_df):\n",
    "    \"\"\"Applies the \"assign-season\" function to the DataFrame to create a new \"Season\" column \"\"\"\n",
    "    shark_df['Season'] = shark_df.apply(assign_season, axis=1)\n",
    "    return shark_df\n",
    "\n",
    "def clean_activity(df: pd.DataFrame):\n",
    "    \"\"\"Cleans the \"Activity\" column \"\"\"\n",
    "    \n",
    "    version_2 = df.copy()\n",
    "    \n",
    "    #converts to lower case\n",
    "    version_2.loc[:, \"Activity\"] = version_2[\"Activity\"].str.lower()\n",
    "\n",
    "    #version_2 = version_2.dropna(subset=[\"Activity\"])\n",
    "\n",
    "    #count activities\n",
    "    activities = version_2[\"Activity\"].value_counts()\n",
    "    threshold = 5\n",
    "    new_activities = activities[activities >= threshold].index\n",
    "\n",
    "     # Replace NaN with \"Unknown\"\n",
    "    version_2[\"Activity\"] = version_2[\"Activity\"].fillna(\"unknown\")\n",
    "\n",
    "    # Replace activities below threshold with \"Unknown\"\n",
    "    version_2.loc[~version_2[\"Activity\"].isin(new_activities), \"Activity\"] = \"unknown\"\n",
    "\n",
    "\n",
    "    acivity_corrections = {\"swimming\": \"swimming\", \"bathing\": \"swimming\",\"treading water\": \"swimming\", \" swimming\" : \"swimming\", \"floating on his back\": \"swimming\", \"swimming \": \"swimming\", \"freedom swimming\":\"swimming\",\n",
    "                       \"free diving\": \"diving\", \"free diving for abalone\" : \"diving\",\n",
    "                       \"diving for trochus\" : \"diving\", \"skindiving\" : \"diving\",\n",
    "                       \"spearfishing on scuba\": \"spearfishing\", \"spearfishing / free diving\": \"spearfishing\", \"spearfishing\": \"spearfishing\",\n",
    "                       \" spearfishing\" : \"spearfishing\", \"spearfishing \" : \"spearfishing\",\n",
    "                       \"diving for abalone\": \"diving\", \"unknown\":\"unknown\", \n",
    "                       \"stand-up paddleboarding\": \"paddleboarding\", \"paddleboarding\":\"paddleboarding\",\n",
    "                        \"surf skiing \": \"surf ski\", \"surf-skiing\": \"surf ski\", \"surf skiing\":\"surf ski\",\" surf skiing\":\"surf ski\", \"surf skiing \":\"surf ski\",\n",
    "                        \"sitting on surfboard\" : \"surfing\", \"surfing\":\"surfing\", \"surfing (sitting on his board)\" : \"surfing\", \"surfing \": \"surfing\", \" surfing\": \"surfing\",\n",
    "                        \"fishing for mackerel\": \"fishing\", \"wade fishing\" : \"fishing\",\n",
    "                        \"body boarding\": \"body surfing\", \"freediving\": \"diving\",\n",
    "                        \"walking\": \"wading\", \"wading\": \"wading\", \"standing\": \"wading\",\n",
    "                       \"scuba diving\": \"diving\", \"diving\": \"diving\", \"pearl diving\" : \"diving\", \"snorkeling\": \"diving\",\n",
    "                       \"kayak fishing\": \"fishing\", \"fishing for sharks\" : \"fishing\", \"shark fishing\" : \"fishing\",\n",
    "                        \"hard hat diving\": \"diving\", \"paddleskiing\" : \"paddle boarding\", \"fell overboard\" : \"sailing\",\n",
    "                       \"floating\" : \"swimming\", \"fishing on a boat\": \"fishing\", \"surf fishing\": \"fishing\", \" fishing\": \"fishing\", \"fishing \": \"fishing\"\n",
    "                        }\n",
    "    version_2[\"Activity\"] = version_2[\"Activity\"].replace(acivity_corrections) # apply corrections\n",
    "    version_2[\"Activity\"] = version_2[\"Activity\"].str.title()\n",
    "\n",
    "    return version_2\n",
    "\n",
    "def clean_fatal(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Create a mapping for the valid values in the \"Fatal\" column \"\"\"\n",
    "    valid_values = {\n",
    "        'Y': 'Y',\n",
    "        'N': 'N',\n",
    "        'UNKNOWN': 'UNKNOWN',\n",
    "        'n': 'N',\n",
    "        ' N': 'N',\n",
    "        'Nq': 'N',\n",
    "        'F': 'UNKNOWN',  # Assuming 'F' means fatal but is not standardized\n",
    "        '': 'UNKNOWN'  # Any empty strings to UNKNOWN\n",
    "    }\n",
    "    \n",
    "    # Replace the values using the mapping\n",
    "    df['Fatal'] = df['Fatal'].replace(valid_values)\n",
    "    \n",
    "    # Optional: Handle any remaining invalid entries by setting them to 'UNKNOWN'\n",
    "    df['Fatal'] = df['Fatal'].where(df['Fatal'].isin(['Y', 'N', 'UNKNOWN']), 'UNKNOWN')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cleaning(df):\n",
    "    \"\"\" Runs all the cleaning functions on the DataFrame to return a cleaned version \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2 = clean_dates2(df2)\n",
    "    df2 = clean_country(df2)\n",
    "    df2 = clean_type(df2)\n",
    "    df2 = clean_states(df2)\n",
    "    df2 = clean_age(df2)\n",
    "    df2 = clean_sex(df2)\n",
    "    df2 = clean_cols(df2)\n",
    "    df2 = age_groups(df2)\n",
    "    df2 = hemisphere(df2)\n",
    "    df2 = add_month(df2)\n",
    "    df2 = add_season(df2)\n",
    "    df2 = clean_activity(df2)\n",
    "    df2 = clean_fatal(df2)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f60cb-bb87-491c-9e8b-40c8a7c6baf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
